\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{amsmath}

\geometry{margin=1in}

% Colors for code and links
\definecolor{primary}{RGB}{102, 126, 234}
\definecolor{secondary}{RGB}{118, 75, 162}
\definecolor{codebackground}{RGB}{245, 245, 245}

\hypersetup{
    colorlinks=true,
    linkcolor=secondary,
    filecolor=magenta,      
    urlcolor=primary,
    pdftitle={Real-time Disaster RAG Platform Technical Overview},
}

\titleformat{\section}{\large\bfseries\color{secondary}}{}{0em}{\thesection.~}
\titleformat{\subsection}{\bfseries\color{primary}}{}{0em}{\thesubsection.~}

\title{
    \vspace{-1in}
    \Huge \textbf{Real-time Disaster RAG Platform} \\
    \Large Technical Overview \& Architectural Design
}
\author{Wiam Lachqer, Ossama Oualy, Othmane BELHAJ}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}
The Real-time Disaster Retrieval-Augmented Generation (RAG) platform is a state-of-the-art system designed to monitor, ingest, and analyze global disaster events in real-time. By combining high-velocity data streaming via Apache Kafka with specialized small-parameter language models (SLMs), the platform provides immediate, context-aware insights into natural disasters such as earthquakes, floods, and wildfires.

\section{Streaming Architecture \& Ingestion}
The platform's "streaming-first" philosophy is powered by a high-availability Apache Kafka cluster, ensuring zero data loss and sub-second processing latency.

\subsection{Back-to-Back Kafka Pipelines}
The system implements a two-stage message pipeline:
\begin{enumerate}
    \item \textbf{Raw Ingestion}: Specialized producers poll official APIs (USGS, GDACS, NASA) every 180 seconds, pushing raw JSON payloads into the \texttt{raw-events} topic.
    \item \textbf{Stream Processing}: A normalizer service consumes from \texttt{raw-events}, applies schema validation, and calculates severity levels. The sanitized output is then produced to the \texttt{processed-events} topic.
\end{enumerate}

\subsection{Consumer Group Management}
To ensure persistent reliability, the \textbf{Embedding Builder} operates within a dedicated Kafka consumer group (\texttt{embedding-group}). This allows the system to:
\begin{itemize}
    \item \textbf{Auto-Commit Offsets}: Track processing progress, ensuring that if a service restarts, it resumes exactly where it left off.
    \item \textbf{Rebalance Latency}: Distribute event loads across multiple instances in high-traffic scenarios.
\end{itemize}

\section{The Vector Engine \& Chronological Intelligence}
The retrieval accuracy relies on a unified embedding strategy to bridge the gap between structured JSON and natural language queries.

\subsection{Unified Document Synthesis}
A crucial innovation of the platform is the \textbf{Synthesis Strategy}. Before embedding, every event is converted into a keyword-boosted natural language document. By prioritizing the \textit{Event Type} and \textit{Location} at the start of the string, we maximize the semantic weight of these key entities during the Transformer's attention phase.

\subsection{Meta-Sorting \& Feeds}
Traditional vector databases often retrieve by relevance only, ignoring the temporal decay of disaster data. The platform implements a \textbf{Chronological Re-ranker} for the "Latest Arrivals" feed. The API fetches a candidate pool from ChromaDB and performs a stable metadata sort based on ISO-8601 timestamps, ensuring the user always sees the most recent anomalies across all heterogeneous sources (USGS, GDACS, NWS).

\section{Hybrid RAG Implementation}
The core intelligence of the platform is driven by a multi-stage, dual-path RAG pipeline.

\subsection{Fetch Live: Bridging the "Retrieval Gap"}
To handle the most immediate events (occurring seconds before the query), the platform implements \textbf{Fetch Live} functionality. When triggered, the system initiates a parallel on-demand fetch:
\begin{itemize}
    \item \textbf{Path A (Stored)}: Standard KNN search in the vectorized 300+ event database.
    \item \textbf{Path B (Live)}: Direct REST polling of upstream feeds, followed by sub-second embedding injection.
\end{itemize}
These paths are merged and de-duplicated at the retrieval boundary, ensuring the LLM has a complete view of both historical and instantaneous context.

\subsection{Context Enrichment \& Generation}
Before generation, the \textbf{NewsEnricher} queries the GDELT API. The final context is passed to the \textbf{Qwen-0.6B} model via Ollama. The model is constrained by strict prompt engineering to prioritize the official data sections.

\section{Self-Healing Pipeline \& Orchestration}
To ensure 24/7 availability, the platform implements a \textbf{Supervisor Pattern} in the main orchestrator:
\begin{itemize}
    \item \textbf{Process Monitoring}: A master watchdog monitors the health of ingestion, processing, and API services every 5 seconds.
    \item \textbf{Auto-Recovery}: Any crashed component is automatically re-instantiated with its previous state, maintaining the Kafka consumer group offset.
\end{itemize}

\section{Conclusion}
By integrating Kafka-driven streaming with a high-precision vector engine and specialized generation constraints, this platform transforms heterogeneous disaster data into reliable, actionable intelligence. The autonomous supervisor ensures that even in extreme network conditions, the pulse of global monitoring remains uninterrupted.

\end{document}
